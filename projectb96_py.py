# -*- coding: utf-8 -*-
"""projectb96.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EhedGEyssOs5JAAfO6opwhMxogDc9ejH
"""



import pandas as pd # Python library for data analysis and data frame
import numpy as np # Numerical Python library for linear algebra and computations

# Visualisation libraries
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['figure.figsize'] = (12, 8); # setting the figuresize 

from datetime import datetime, date #Library to deal with datetime columns

# time series analysis libraries
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# time series forecasting libraries
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
!pip install pmdarima
from pmdarima.arima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly

# Neural network required libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

# model evaluation libraries
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

#for saving trained model
import pickle

import warnings
warnings.filterwarnings("ignore") # To prevent kernel from showing any warning

df = pd.read_csv('/content/DayForecast.csv')

df

df.info()

df['Date'] = pd.to_datetime(df['Date'])

df.info()

df.set_index(['Date'], inplace=True) # changing index to datetime

df.head()

df.index.min(), df.index.max()

# checking if data is continues
if (df.index.max() - df.index.min() == 214): 
    print('Data Is Continuous')

# Checking Whether data is normally distributed
import scipy. stats as stats
import pylab
stats.probplot(df. Quantity, dist="norm", plot=pylab)

df.plot(grid=True);

"""Data has some seasonality, and trend is present and maybe cyclicity too"""

#split the data into train and test
size = int(len(df)*0.70)
#splitting the first 70% data zto train and the rest to test

train_df = df[:size]
train_df.tail()

test_df = df[size:]
test_df.head()

train_df. shape

test_df. shape

"""Time Series Decomposition:
A time series is usually composed of the following components:

1) Trend : This component usually is increasing, decreasing, or constant.
2) Seasonality : This is the periodic behavior of the time series that occurs within a year.
3) Residual : This is what remains of the time series after the trend and seasonality are removed.

The basic approach to seasonal decomposition splits the time series into above components.
"""

decompose_add = seasonal_decompose(df['Quantity'])
decompose_add.plot();

"""Stationary Time Series :
A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., "stationarized") through the use of mathematical transformations. A stationarized series is relatively easy to predict

Sign of obvious trends, seasonality, or other systematic structures in the series are indicators of a non-stationary series. A more accurate method would be to use a statistical test, such as the Dickey-Fuller test.

ADFuller Test:
If Test statistic < Critical Value and p-value < 0.05 â€“ then series is stationary
"""

# functon for adf test
def adf_test(timeseries):
    print ('Results of Dickey-Fuller Test:')
    print ('----------------------------------------------')
    adftest = adfuller(timeseries)
    adf_output = pd.Series(adftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in adftest[4].items():
        adf_output['Critical Value (%s)'%key] = value
    print (adf_output)
    
# calling adf function and passing series
adf_test(df.values)

"""Observation ðŸ‘€
The p-value obtained is less than significance level of 0.05 and the ADF statistic is less than any of the critical values. Hence the series is stationary

Autocorrelation and Partial Autocorrelation Function:
Autocorrelation and partial autocorrelation are plots that graphically summarize the impact of observations at prior timesteps on the observations we are trying to predict.

ACF plot gives the q value and PACF gives the p value
Look for tail of pattern in either ACF or PACF. If tail is crossing the blue region then it will give us potential p and q values.
"""

# if the series is not stationary then make sure to pass differenced series instead of original series
plot_acf(df);
plot_pacf(df);

"""Observation ðŸ‘€
1.From ACF plot we can see that q values can be 1, 7 & 24
2.From PACF plot we can see that p values can be 1,7, & 24

Exponential Smoothing:
Exponential smoothing is a time series forecasting method for univariate data. There are three main types of exponential smoothing time series forecasting methods. A simple method that assumes no systematic structure, an extension that explicitly handles trends, and the most advanced approach that add support for seasonality.

Single Exponential Smoothing

Single Exponential Smoothing
Single Exponential Smoothing, SES for short, also called Simple Exponential Smoothing, is a time series forecasting method for univariate data without a trend or seasonality.
"""

single_exp = SimpleExpSmoothing(train_df).fit()
single_exp_train_pred = single_exp.fittedvalues
single_exp_test_pred = single_exp.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
single_exp_test_pred.plot(color='b', legend=True, label='Prediction')

print('Train RMSE:',mean_squared_error(train_df, single_exp_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, single_exp_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, single_exp_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, single_exp_test_pred))

"""Double Exponential Smoothing
Double Exponential Smoothing is an extension to Exponential Smoothing that explicitly adds support for trends in the univariate time series
"""

double_exp = ExponentialSmoothing(train_df, trend=None, initialization_method='heuristic', seasonal='add', seasonal_periods=29, damped_trend=False).fit()
double_exp_train_pred = double_exp.fittedvalues
double_exp_test_pred = double_exp.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
double_exp_test_pred.plot(color='b', legend=True, label='prediction')
plt.show()

print('Train RMSE:',mean_squared_error(train_df, double_exp_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, double_exp_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, double_exp_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, double_exp_test_pred))

"""Triple Exponential Smoothing
Triple Exponential Smoothing is an extension of Exponential Smoothing that explicitly adds support for seasonality to the univariate time series. Also known as Holt-Winters Exponential Smoothing.
"""

hw_model = ExponentialSmoothing(train_df['Quantity'],
                          trend    ='add',
                          initialization_method='heuristic',
                          seasonal = "add", 
                          seasonal_periods=28, 
                          damped_trend=True).fit()
hw_train_pred =  hw_model.fittedvalues
hw_test_pred =  hw_model.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
hw_test_pred.plot(color='b', legend=True, label='prediction')
plt.show()

print('Train RMSE:',mean_squared_error(train_df, hw_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, hw_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, hw_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, hw_test_pred))

"""ARIMA
A popular and widely used statistical method for time series forecasting is the ARIMA model. ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. The parameters of the ARIMA model are defined as follows:

p : The number of lag observations included in the model, also called the lag order.
d : The number of times that the raw observations are differenced, also called the degree of differencing.
q : The size of the moving average window, also called the order of moving average.
"""

# we got the p,d,q value from time series analysis
ar = ARIMA(train_df, order=(7,1,7)).fit()
ar_train_pred = ar.fittedvalues
ar_test_pred = ar.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
ar_test_pred.plot(color='b', legend=True, label='prediction')
plt.show()

print('Train RMSE:',mean_squared_error(train_df, ar_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, ar_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, ar_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, ar_test_pred))

"""Grid Search p,d,q Values
From acf and pacf we got different potenstial p,q values. Lets use grid search to find the best pair out of them
"""

# evaluate an ARIMA model for a given order (p,d,q)
def evaluate_arima_model(arima_order):
    # prepare training dataset
    train_df = df[:size]
    test_df = df[size:]
    # make predictions
    model = ARIMA(train_df, order=arima_order).fit()
    model_pred = model.forecast(110)
    rmse = (mean_squared_error(test_df, model_pred)**0.5)
    return rmse

# evaluate combinations of p, d and q values for an ARIMA model
def evaluate_models(dataset, p_values, q_values):
	dataset = dataset.astype('float32')
	best_score, best_cfg = float("inf"), None
	for p in p_values:
		for q in q_values:
				order = (p,1,q)
				try:
					rmse = evaluate_arima_model(order)
					if rmse < best_score:
						best_score, best_cfg = rmse, order
					
				except:
					continue
	print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))

# evaluate parameters
p_values = [1,7,24]
q_values = [1,7,24]

evaluate_models(df['Quantity'].values, p_values, q_values)

arima = ARIMA(train_df['Quantity'], order=(7,1,7)).fit()
arima_train_pred = arima.predict()
arima_test_pred = arima.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
arima_test_pred.plot(color='b', legend=True, label='prediction')
plt.show()

print('Train RMSE:',mean_squared_error(train_df, arima_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, arima_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, arima_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, arima_test_pred))

"""Auto ARIMA
Although ARIMA is a very powerful model for forecasting time series data, the data preparation and parameter tuning processes end up being really time consuming. Before implementing ARIMA, you need to make the series stationary, and determine the values of p and q using the plots we discussed above. Auto ARIMA makes this task really simple for us as it eliminates many steps
"""

auto_arima(df['Quantity'], m=30, max_order= None, max_p=16, max_q=16, max_d=1,max_P=16, max_Q=16, max_D=2, maxiter=50, alpha=0.05, n_jobs=-1,information_criterion='aic', out_of_sample_size=30).summary()

# pass the order and seasonal order values we got from auto arima
sarima = SARIMAX(train_df['Quantity'], order=(0,1,0), seasonal_order=(1,0,0,30)).fit()
sarima_train_pred = sarima.predict()
sarima_test_pred = sarima.forecast(110)

train_df['Quantity'].plot(style='--', color='gray', legend=True, label='train_df')
test_df['Quantity'].plot(style='--', color='r', legend=True, label='test_df')
sarima_test_pred.plot(color='b', legend=True, label='prediction')
plt.show()

print('Train RMSE:',mean_squared_error(train_df, sarima_train_pred)**0.5)
print('Test RMSE:',mean_squared_error(test_df, sarima_test_pred)**0.5)
print('Train MAPE:',mean_absolute_percentage_error(train_df, sarima_train_pred))
print('Test MAPE:',mean_absolute_percentage_error(test_df, sarima_test_pred))

"""Prophet

Prophet
The Prophet library is an open-source library designed for making forecasts for univariate time series datasets. It is easy to use and designed to automatically find a good set of hyperparameters for the model in an effort to make skillful forecasts for data with trends and seasonal structure by default.

The DataFrame must have a specific format. The first column must have the name â€˜dsâ€˜ and contain the date-times. The second column must have the name â€˜yâ€˜ and contain the observations.
"""

# converting the original dataframe into required format by prophet
prophet_df = df.copy()
prophet_df.reset_index(inplace=True)
prophet_df.columns=['ds','y']
prophet_df.head()

# train test split data
prophet_train_df = prophet_df.iloc[:-110]
prophet_test_df = prophet_df.iloc[-110:]

prophet_model = Prophet()
prophet_model.fit(prophet_train_df) #fit training data to model

future = prophet_model.make_future_dataframe(periods=23, freq='YS')
prophet_predictions = prophet_model.predict(future)

prophet_predictions.tail()

plot_plotly(prophet_model, prophet_predictions)

prophet_train_df['prophet_train_pred'] = prophet_predictions.iloc[:-110]['yhat']
prophet_test_df['prophet_test_pred'] = prophet_predictions.iloc[-110:]['yhat']

comparision_df = pd.DataFrame(data=[['Single exp smoothing', 41.89 , 0.37],
                           ['double exp smoothing', 46.05,  0.34],
                          ['Triple exp smoothing', 42.07, 0.34],
                          ['ARIMA(15,1,15)', 42.071, 0.36],
                          ['ARIMA(16,1,16)', 42.07, 0.36],
                          ['SARIMA(0,1,0)(1,0,0,30)', 45.31, 0.32]],columns=['Model','RMSE','MAPE'])

comparision_df.set_index('Model', inplace=True)

comparision_df.sort_values(by='MAPE')

double_exp = ExponentialSmoothing(df, trend=None, initialization_method='heuristic', seasonal='add', seasonal_periods=29, damped_trend=False).fit()

double_exp_pred = double_exp.forecast(120)

double_exp_pred

df['Quantity'].plot(style='--', color='gray', legend=True, label='known')
double_exp_pred.plot(color='b', legend=True, label='prediction')
plt.show()

pickle.dump(double_exp, open('forecast_model_doubleexp.pickle','wb'))

!pip install streamlit

import pickle
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import warnings
warnings.filterwarnings("ignore")

#load the model
model = pickle.load(open('/content/forecast_model_doubleexp.pickle','rb'))

#load dataset to plot alongside predictions
df = pd.read_csv('/content/DayForecast.csv')
df['Date'] = pd.to_datetime(df['Date'])
df.set_index(['Date'], inplace=True)

#page configuration
st.set_page_config(layout='centered')
image = Image.open('/content/project image].jpg')
st.image(image)

date = st.slider("Select number of dates",1,30,step = 1)

pred = model.forecast(date)
pred = pd.DataFrame(pred, columns=['Quantity'])

if st.button("Predict"):

        col1, col2 = st.columns([2,3])
        with col1:
             st.dataframe(pred)
        with col2:
            fig, ax = plt.subplots()
            df['Quantity'].plot(style='--', color='gray', legend=True, label='known')
            pred['Quantity'].plot(color='b', legend=True, label='prediction')
            st.pyplot(fig)

# Run the Streamlit app
if __name__ == '__model__':
    main()